{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -d vision_transformer ] || git clone --depth=1 https://github.com/google-research/vision_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: jax 0.3.25 does not provide the extra 'gpu'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qr vision_transformer/vit_jax/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 19:15:34.498166: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-17 19:15:34.641474: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-17 19:15:35.247541: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-17 19:15:35.247620: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-17 19:15:35.247627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Import files from repository.\n",
    "\n",
    "# Provide OOM errors\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "import sys\n",
    "if './vision_transformer' not in sys.path:\n",
    "  sys.path.append('./vision_transformer')\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import jax\n",
    "from torchvision import transforms as T\n",
    "from torchvision import datasets\n",
    "import torch as th\n",
    "\n",
    "from vit_jax import checkpoint\n",
    "from vit_jax import models\n",
    "from vit_jax.configs import models as models_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_model_jit(model, params):\n",
    "  return jax.jit(lambda x: model.apply({\"params\": params}, x, train=False))\n",
    "\n",
    "def get_accuracy(model, params, dataset, batch_size):\n",
    "  run_model_jit = get_run_model_jit(model, params)\n",
    "  dataloader = th.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  good = 0\n",
    "  total = 0\n",
    "  tqdm_loader = tqdm(dataloader, desc=\"Evaluating\", unit=\"batches\")\n",
    "  for images, labels in tqdm_loader:\n",
    "    images = images.permute(0, 2, 3, 1).numpy()\n",
    "    logits = run_model_jit(images)\n",
    "\n",
    "    good  += jax.device_get(jax.numpy.equal(logits.argmax(axis=1), labels.numpy()).sum())\n",
    "    total += labels.shape[0]\n",
    "\n",
    "    tqdm_loader.set_description_str(f\"Accuracy: {good / total:.2%}\")\n",
    "\n",
    "  return good / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "  T.ToTensor(),\n",
    "  T.Resize((384, 384)),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageNet(root=\"ImageNet\", split=\"val\", transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 19:15:37.421818: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331.4 MiB - gs://vit_models/imagenet21k+imagenet2012/ViT-B_16.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be002824f7f47339083012e4d04e733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/6250 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-B_16 w/ Imagenet21k accuracy: 81.56%\n"
     ]
    }
   ],
   "source": [
    "model_config = models_config.MODEL_CONFIGS[\"ViT-B_16\"]\n",
    "path = \"gs://vit_models/imagenet21k+imagenet2012/ViT-B_16.npz\"\n",
    "print(f'{tf.io.gfile.stat(path).length / 1024 / 1024:.1f} MiB - {path}')\n",
    "\n",
    "model = models.VisionTransformer(num_classes=1000, **model_config)\n",
    "params = checkpoint.load(path)\n",
    "accuracy_b_16 = get_accuracy(model, params, dataset, 8)\n",
    "print(f\"ViT-B_16 w/ Imagenet21k accuracy: {accuracy_b_16:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1162.5 MiB - gs://vit_models/imagenet21k+imagenet2012/ViT-L_16.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86e7db83e81420ea5428c4b7cedec1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/6250 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mVisionTransformer(num_classes\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_config)\n\u001b[1;32m      6\u001b[0m params \u001b[39m=\u001b[39m checkpoint\u001b[39m.\u001b[39mload(path)\n\u001b[0;32m----> 7\u001b[0m accuracy_b_16 \u001b[39m=\u001b[39m get_accuracy(model, params, dataset, \u001b[39m8\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mViT-L_16 w/ Imagenet21k accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy_b_16\u001b[39m:\u001b[39;00m\u001b[39m.2%\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [4], line 15\u001b[0m, in \u001b[0;36mget_accuracy\u001b[0;34m(model, params, dataset, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     13\u001b[0m logits \u001b[39m=\u001b[39m run_model_jit(images)\n\u001b[0;32m---> 15\u001b[0m good  \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mdevice_get(jax\u001b[39m.\u001b[39;49mnumpy\u001b[39m.\u001b[39;49mequal(logits\u001b[39m.\u001b[39;49margmax(axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), labels\u001b[39m.\u001b[39;49mnumpy())\u001b[39m.\u001b[39;49msum())\n\u001b[1;32m     16\u001b[0m total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m tqdm_loader\u001b[39m.\u001b[39mset_description_str(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{\u001b[39;00mgood \u001b[39m/\u001b[39m total\u001b[39m:\u001b[39;00m\u001b[39m.2%\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ViT/lib/python3.9/site-packages/jax/_src/api.py:3077\u001b[0m, in \u001b[0;36mdevice_get\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   3075\u001b[0m \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m tree_leaves(x):\n\u001b[1;32m   3076\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3077\u001b[0m     y\u001b[39m.\u001b[39;49mcopy_to_host_async()\n\u001b[1;32m   3078\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m   3079\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_config = models_config.MODEL_CONFIGS[\"ViT-L_16\"]\n",
    "path = \"gs://vit_models/imagenet21k+imagenet2012/ViT-L_16.npz\"\n",
    "print(f'{tf.io.gfile.stat(path).length / 1024 / 1024:.1f} MiB - {path}')\n",
    "\n",
    "model = models.VisionTransformer(num_classes=1000, **model_config)\n",
    "params = checkpoint.load(path)\n",
    "accuracy_b_16 = get_accuracy(model, params, dataset, 8)\n",
    "print(f\"ViT-L_16 w/ Imagenet21k accuracy: {accuracy_b_16:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('ViT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fca5c79e43e2fcfb66e9549e37c2cae24b0d67edb4a3690fb9aca4f225bdf87a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
